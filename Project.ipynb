{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061ce801",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866fcd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "# import dlib to deal with face recognition and lip detection for images\n",
    "import dlib #(pip install dlib or create a virtual conda environement to install dlib)\n",
    "import cv2 #(pip install opencv-python)\n",
    "import time\n",
    "import shutil\n",
    "import imageio.v2 as imageio\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import torch\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688536f",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac232ce9",
   "metadata": {},
   "source": [
    "![Image of Keys to Words](miscellaneous/Key_Words_Data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of all the people speaking in the dataset\n",
    "people = ['F01','F02','F04','F05','F06','F07','F08','F09', 'F10','F11','M01','M02','M04','M07','M08']\n",
    "# the data types we're dealing with \n",
    "data_types = ['words']\n",
    "folder_enum = ['01','02','03','04','05','06','07','08', '09', '10']\n",
    "instances = ['01','02','03','04','05','06','07','08', '09', '10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that connect the words one is saying to an integer\n",
    "words = ['Begin', 'Choose', 'Connection', 'Navigation', 'Next', 'Previous', 'Start', 'Stop', 'Hello', 'Web']          \n",
    "words_di = {i:words[i] for i in range(len(words))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19306e",
   "metadata": {},
   "source": [
    "# Extraction of lip image from a body image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b280b2a",
   "metadata": {},
   "source": [
    "You must first download dataset from: |![Link to dataset](https://www.kaggle.com/datasets/apoorvwatsky/miraclvc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the body image from the image path, detect the face, and extract the lip image from the face\n",
    "def crop_and_save_image(img_path, write_img_path, img_name):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('dlib_shape_predictor_model/shape_predictor_68_face_landmarks.dat')\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "\n",
    "    # read image and tranform it into grayscale\n",
    "    image = cv2.imread(img_path)\n",
    "    image = imutils.resize(image, width=500)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    if len(rects) > 1:\n",
    "    \tprint( \"ERROR: more than one face detected\")\n",
    "    \treturn\n",
    "    if len(rects) < 1:\n",
    "    \tprint( \"ERROR: no faces detected\")\n",
    "    \treturn\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        name, i, j = 'mouth', 48, 68\n",
    "\n",
    "\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))        \n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi = imutils.resize(roi, width = 250, inter=cv2.INTER_CUBIC) \n",
    "        cv2.imwrite('lip_cropped/' + write_img_path, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb85941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_folder = 'lip_cropped'\n",
    "os.mkdir(save_folder)\n",
    "i = 1\n",
    "for person_ID in people:\n",
    "    start_time = time.time()\n",
    "    if not os.path.exists(f'{save_folder}/{person_ID}' ):\n",
    "        os.mkdir(f'{save_folder}/{person_ID}/')\n",
    "\n",
    "    for data_type in data_types:\n",
    "        if not os.path.exists(f'{save_folder}/{person_ID}/{data_type}'):\n",
    "            os.mkdir(f'{save_folder}/{person_ID}/{data_type}')\n",
    "\n",
    "        for phrase_ID in folder_enum:\n",
    "            if not os.path.exists(f'{save_folder}/{person_ID}/{data_type}/{phrase_ID}'):\n",
    "\n",
    "                os.mkdir(f'{save_folder}/{person_ID}/{data_type}/{phrase_ID}')\n",
    "\n",
    "            for instance_ID in instances:\n",
    "                directory = 'dataset' + \"/\" + person_ID + \"/\" + data_type + \"/\" + phrase_ID + \"/\" + instance_ID + \"/\"\n",
    "                dir_temp = person_ID + \"/\" + data_type + \"/\" + phrase_ID + \"/\" + instance_ID + \"/\"\n",
    "                filelist = os.listdir(directory)\n",
    "                if not os.path.exists(f'{save_folder}/{person_ID}/{data_type}/{phrase_ID}/{instance_ID}'):\n",
    "                    os.mkdir(f'{save_folder}/{person_ID}/{data_type}/{phrase_ID}/{instance_ID}')\n",
    "\n",
    "                    for img_name in filelist:\n",
    "                        if img_name.startswith('color'):\n",
    "                            crop_and_save_image(directory + '' + img_name,\n",
    "                                                dir_temp + '' + img_name, img_name)\n",
    "    end_time = time.time()                     \n",
    "    print(f'Iteration : {i}. Time taken: {end_time - start_time}')\n",
    "    i += 1                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4af6ee",
   "metadata": {},
   "source": [
    "#### Cropped lip images of a person saying a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8a7f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for images in os.listdir('lip_cropped/F01/words/01/01'):\n",
    "    plt.figure()\n",
    "    plt.imshow(cv2.imread(f'lip_cropped/F01/words/01/01/{images}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88681600",
   "metadata": {},
   "source": [
    "# Feature extraction and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 22\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "MAX_WIDTH = 100\n",
    "MAX_HEIGHT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split between train set and test set\n",
    "test_ds = random.sample(people, 2)\n",
    "train_ds = people.copy()\n",
    "for test in test_ds:\n",
    "    train_ds.remove(test)\n",
    "val_ds = random.sample(train_ds, 1)\n",
    "train_ds.remove(val_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330f663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "for person_id in people:\n",
    "    tx1 = time.time()\n",
    "    for data_type in data_types:\n",
    "        for word_index, word in enumerate(folder_enum):\n",
    "            print(f\"Word : '{words[word_index]}'\")\n",
    "            for iteration in instances:\n",
    "                path = os.path.join('lip_cropped', person_id, data_type, word, iteration)\n",
    "                filelist = sorted(os.listdir(path + '/'))\n",
    "                sequence = [] \n",
    "                for img_name in filelist:\n",
    "                    if img_name.startswith('color'):\n",
    "                        image = imageio.imread(path + '/' + img_name)\n",
    "                        image = resize(image, (MAX_WIDTH, MAX_HEIGHT))\n",
    "                        image = 255 * image\n",
    "                        # Convert to integer data type pixels.\n",
    "                        image = image.astype(np.uint8)\n",
    "                        sequence.append(image)\n",
    "                # adding padding to create samesequence of the same size\n",
    "                pad_array = [np.zeros((MAX_WIDTH, MAX_HEIGHT))]     \n",
    "                # addding paddings of zeros\n",
    "                sequence.extend(pad_array * (max_seq_length - len(sequence)))\n",
    "                sequence = np.array(sequence)\n",
    "                                \n",
    "                if person_id in test_ds:\n",
    "                    X_test.append(sequence)\n",
    "                    y_test.append(word_index)\n",
    "                elif person_id in val_ds:\n",
    "                    X_val.append(sequence)\n",
    "                    y_val.append(word_index)\n",
    "                else:\n",
    "                    X_train.append(sequence)\n",
    "                    y_train.append(word_index)  \n",
    "    tx2 = time.time()\n",
    "    print(f'Finished reading images for person {person_id}. Time taken : {tx2 - tx1} secs.')    \n",
    "    \n",
    "t2 = time.time()\n",
    "print(f\"Time taken for creating constant size 3D Tensors from the cross lip images : {t2 - t1} secs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30ec61",
   "metadata": {},
   "source": [
    "##### Normalization of X_value to fit between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    v_min = X.min(axis=(2, 3), keepdims=True)\n",
    "    v_max = X.max(axis=(2, 3), keepdims=True)\n",
    "    X = (X - v_min)/(v_max - v_min)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of X values\n",
    "X_train = normalize(np.array(X_train))\n",
    "X_val = normalize(np.array(X_val))\n",
    "X_test = normalize(np.array(X_test))\n",
    "\n",
    "# Shuffle x and corresponding y value for better training\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=0)\n",
    "X_val, y_val = shuffle(X_val, y_val, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=4)\n",
    "X_val = np.expand_dims(X_val, axis=4)\n",
    "X_test = np.expand_dims(X_test, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f949768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, ZeroPadding3D, TimeDistributed, LSTM, GRU, Reshape\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "y_val = to_categorical(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d40fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1st layer group\n",
    "model.add(Conv3D(64, (3, 3, 3), strides = 1, input_shape=(22, 100, 100, 1), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(128, (3, 3, 3), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add(Conv3D(256, (2, 2, 2), activation='relu', strides=1))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "\n",
    "model.add((Flatten()))\n",
    "\n",
    "# # Functional Connection Layer\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "# Output return\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=45)\n",
    "t2 = time.time()\n",
    "print()\n",
    "print(f\"Training time : {t2 - t1} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert from arrays to torch tensor\n",
    "# train_input = torch.tensor(X_train)\n",
    "# train_output = torch.tensor(y_train)\n",
    "# validation_input = torch.tensor(X_val)\n",
    "# validation_output = torch.tensor(y_val)\n",
    "# test_input = torch.tensor(X_test)\n",
    "# test_output = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d68c5",
   "metadata": {},
   "source": [
    "##### Load train, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd79397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LipCroppedDataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         super(Dataset)\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load train and test_dataset\n",
    "# train_dataset = LipCroppedDataset(train_input, train_output)\n",
    "# test_dataset = LipCroppedDataset(test_input, test_output)\n",
    "# validation_dataset = LipCroppedDataset(validation_input, validation_output)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "#                                           batch_size = 500,\n",
    "#                                           shuffle = True) \n",
    "# test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "#                                           batch_size = len(test_dataset),\n",
    "#                                           shuffle = True)\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset = validation_dataset,\n",
    "#                                                batch_size = len(validation_dataset),\n",
    "#                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32687fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv3d(22, 64, 3, stride = 1)\n",
    "#         self.pool = nn.MaxPool3d(2, stride = 2)\n",
    "#         self.conv2 = nn.Conv3d(64 , 128, 3, stride = 1)\n",
    "#         self.conv3 = nn.Conv3d(128, 256, 3, stride = 1)\n",
    "#         self.fc1 = nn.Linear(256 * 3 * 3, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "#         self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.pool(F.relu(self.conv1(x)))\n",
    "#         out = self.pool(F.relu(self.conv2(out)))\n",
    "#         out = self.pool(F.relu(self.conv3(out)))\n",
    "#         out = out.view(-1, 256)\n",
    "#         out = F.relu(self.fc1(out))\n",
    "#         out = F.relu(self.fc2(out))\n",
    "#         out = self.fc3(out)\n",
    "\n",
    "#         return self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ConvNet()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# learning_rate = 0.1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(model, data, optimizer, loss_fnc):\n",
    "#     for inputs, targets in data:\n",
    "        \n",
    "#         inputs = inputs.to(torch.float32)\n",
    "\n",
    "#         # forward pass and calculate loss\n",
    "#         predictions = model(inputs)\n",
    "#         loss = loss_fnc(predictions, targets)\n",
    "        \n",
    "#         # back propagate and update weights\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "#     print(f\"Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83615542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1):\n",
    "#     print(f\"Epoch: {epoch + 1}\")\n",
    "#     train_one_epoch(model, train_dataset, optimizer, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2389b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
